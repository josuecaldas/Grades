---
title: "SM_1"
author: "Josué Caldas"
date: "4/6/2022"
output: html_document
---

# Conexión a GitHub

```{r}
pacman::p_load(devtools, usethis, tidyverse)
use_git_config(user.name = "Josué Caldas", user.email = "josue.caldas@pucp.edu.pe")
usethis::use_git()
```

#### Set up

Instalamos las librerías

```{r}
library(dplyr)
library(plyr)
#library(dplyr)
library(readr)
library(caret)
library(ggplot2)
library(repr)
library(ISLR)
library(haven)
library(jtools)
library(glmnet)
library(broom.mixed)
library(genridge)
library(lmridge)
library(skimr)
library(tidyverse)
library(DataExplorer)
library(scales)
library(corrr)
library(pls)
library(faraway)
library(yarrr)
```

Importamos los datos

```{r}
data = read_dta("grades_transformed.dta")
glimpse(data)
```

Dividimos la data en training set y test set

```{r}
set.seed(2)
index = sample(1:nrow(data), 0.7*nrow(data))

# Creamos el training set
train = data[index,]

# Creamos el test set
test = data[-index,]

# Evaluamos
dim(train)
dim(test)
```

Realizamos la estandarización de variables predictoras numéricas
Ojo, no estamos estandarizando la variable independiente por recomendación de: https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r

```{r}
cols = c("age", "g3")
pre_proc_val <- preProcess(train[,cols], method = c("center", "scale"))

train[, cols] = predict(pre_proc_val, train[, cols])
test[, cols] = predict(pre_proc_val, test[, cols])

summary(train)
```


#### Regresión Lineal

Creamos el modelo lineal

```{r}
lr = lm(g3 ~., data = train)
summary(lr)
```

Creamos la función de evaluación de los modelos

```{r}
eval_metrics = function(model, df, predictions, target){
  resids = df[, target] - predictions
  resids2 = resids**2
  N = length(predictions)
  r2 = as.character(round(summary(model)$r.squared, 4))
  adj_r2 = as.character(round(summary(model)$r.squared, 4))
  print(paste0("adjusted r2: ", adj_r2)) # Adjusted R-squared
  print(paste0("RMSE: ", (as.character(round(sqrt(sum(resids2)/N), 4))))) # RMSE
}
```


Calculamos el Adjusted R squared y el RMSE para el training test

```{r}
predictions = predict(lr, newdata = train)
eval_metrics(lr, train, predictions, target = "g3")
```

Calculamos el Adjusted R squared y el RMSE para el test test

```{r}
predictions = predict(lr, newdata = test)
eval_metrics(lr, test, predictions, target = "g3")
```

```{r}
lr$coefficients
```

Gráfico de los coeficientes de la regresión lineal

```{r}
linear_coefs <- lr$coefficients %>%
                enframe(name = "predictor", value = "coeficiente")

linear_coefs %>%
  filter(predictor != "(intercept)") %>%
  ggplot(aes(x = predictor, y = coeficiente)) +
  geom_col() + 
  ggtitle("Coeficientes del modelo OLS") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(size = 5, angle = 45)) +
  ylim(-0.6, 0.6)

```

```{r}
?labs
```


Exportamos el gráfico a jpg

```{r}
jpeg("coefs_lr.jpeg", width = 12, height = 4, units = 'in', res = 600)
linear_coefs <- lr$coefficients %>%
                enframe(name = "Predictores", value = "Coeficientes")

linear_coefs %>%
  filter(Predictores != "(intercept)") %>%
  ggplot(aes(x = Predictores, y = Coeficientes)) +
  geom_col() +
  ggtitle("Coeficientes del modelo OLS") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(size = 5, angle = 45)) +
  ylim(-0.6, 0.6)
dev.off()
```

### Regresión Ridge

El paquete glmnet necesita una matriz numérica. Aquí se crea mediante la función `dummyVars`

```{r}
variable_names <- names(data)
dummies <- dummyVars(g3 ~., data = data[, variable_names])

train_dummies <- predict(dummies, newdata = train[, variable_names])

test_dummies = predict(dummies, newdata = test[,variable_names])

print(dim(train_dummies)); print(dim(test_dummies))
```

Creamos el training y test set

```{r}
set.seed(2)
# Training set
x_train <- as.matrix(train_dummies)
y_train <- train$g3
#Test set
x_test <- as.matrix(test_dummies)
y_test <- test$g3
```

Definimos el modelo Ridge

```{r}
grid <- 10^seq(10, -2, length = 100)
ridge <- glmnet(x_train, y_train, alpha = 0, lambda = grid)
summary(ridge)
```

Hallamos el lambda óptimo mediante cross validation

```{r}
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0, lambda = grid)
ridge_bestlam <- cv_ridge$lambda.min
ridge_bestlam
par("mar"=c(5,5,5,2))
plot(cv_ridge)
title(main = "Tuning parameter apropiado para modelo Ridge", line = 3)
```


Exportamos el grafico a jpg

```{r}
jpeg("cv_ridge.jpeg", width = 6, height = 4, units = 'in', res = 600)
par("mar"=c(5,5,5,2))
plot(cv_ridge)
title(main = "Tuning parameter apropiado para modelo Ridge", line = 3)
dev.off()
```

Creamos la función de evaluación del modelo Ridge

```{r}
# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

  # Model performance metrics
data.frame(
  RMSE = RMSE,
  Rsquare = R_square
)
  
}
```


Calculamos el Adjusted R squared y el RMSE para el training test


```{r}
prediction_ridge_train <- predict(ridge, s = ridge_bestlam, newx = x_train)
eval_results(y_train, prediction_ridge_train, train)
```

```{r}
prediction_ridge_test <- predict(ridge, s = ridge_bestlam, newx = x_test)
eval_results(y_test, prediction_ridge_test, test)
```


```{r}
plot(ridge, xvar = "lambda") +
  xlim(-5, 10)
```

Exportar a jpg 

```{r}
jpeg("lambda_ridge.jpeg", width = 6, height = 4, units = 'in', res = 600)
plot(ridge, xvar = "lambda")
dev.off()
```

```{r}
# Evolución de los coeficientes en función de lambda
# ==============================================================================
regularizacion <- ridge$beta %>% 
                  as.matrix() %>%
                  t() %>% 
                  as_tibble() %>%
                  mutate(lambda = ridge$lambda)

regularizacion <- regularizacion %>%
                   pivot_longer(
                     cols = !lambda, 
                     names_to = "predictor",
                     values_to = "coeficientes"
                   )

regularizacion %>%
  ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
  geom_line() +
  scale_x_log10(
    breaks = trans_breaks("log10", function(x) 10^x),
    labels = trans_format("log10", math_format(10^.x))
  ) +
  labs(title = "Coeficientes del modelo en función de la regularización") +
  theme_bw() +
  theme(legend.position = "none")
```

 el modelo ridge con el mejor lambda

```{r}
ridge_dos <- glmnet(x = x_train, y = y_train, alpha = 0, lambda = ridge_bestlam)
summary(ridge_dos)
```


```{r}
round(coef(ridge_dos), 4)
```

Gráfico de coeficientes Ridge

```{r}
ridge_coefs <- coef(ridge_dos) %>%
               as.matrix() %>%
               as_tibble(rownames = "predictor") %>%
               dplyr::rename(coeficiente = s0)

ridge_coefs %>%
  filter(predictor != "(Intercept)") %>%
  ggplot(aes(x = predictor, y = coeficiente)) +
  geom_col() +
  labs(title = "Coeficientes del modelo Ridge") +
  theme_bw() +
  theme(axis.text.x = element_text(size = 6, angle = 45)) +
  ylim(-0.6, 0.6)
```

```{r}
jpeg("coefs_ridge.jpeg", width = 12, height = 4, units = 'in', res = 600)
ridge_coefs <- coef(ridge_dos) %>%
               as.matrix() %>%
               as_tibble(rownames = "predictor") %>%
               dplyr::rename(coeficiente = s0)

ridge_coefs %>%
  filter(predictor != "(Intercept)") %>%
  ggplot(aes(x = predictor, y = coeficiente)) +
  geom_col() +
  ggtitle("Coeficientes del modelo Ridge") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(size = 6, angle = 45)) +
  ylim(-0.6, 0.6)
dev.off()
```


### Regresión Lasso

Definimos el modelo Lasso

```{r}
grid_lasso <- 10^seq(2, -3, length = 100)
lasso <- glmnet(x_train, y_train, alpha = 1, lambda = grid_lasso)
#summary(lasso)
```

Calculamos el mejor lambda

```{r}
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1, lambda = grid_lasso)
lasso_bestlam <- cv_lasso$lambda.min
lasso_bestlam
par("mar"=c(5,5,5,2))
plot(cv_lasso)
title(main = "Tuning parameter apropiado para modelo Lasso", line = 3)
```

Exportmos a jpg

```{r}
jpeg("cv_lasso.jpeg", width = 6, height = 4, units = 'in', res = 600)
par("mar"=c(5,5,5,2))
plot(cv_lasso)
title(main = "Tuning parameter apropiado para modelo Lasso", line = 3)
dev.off()
```

Calculamos el RME ajustado y el Rsquared para el training set

```{r}
prediction_lasso_train <- predict(lasso, s = lasso_bestlam, newx = x_train)
eval_results(y_train, prediction_lasso_train, train)
```

```{r}
prediction_lasso_test <- predict(lasso, s = lasso_bestlam, newx = x_test)
eval_results(y_test, prediction_lasso_test, test)
```


```{r}
plot(lasso, xvar = "lambda")
```

Exportamos a jpg

```{r}
jpeg("lambda_lasso.jpeg", width = 6, height = 4, units = 'in', res = 600)
plot(lasso, xvar = "lambda")
dev.off()
```

```{r}
lasso_dos <- glmnet(x = x_train, y = y_train, alpha = 1, lambda = lasso_bestlam)
summary(lasso_dos)
```

```{r}
coef(lasso_dos)
round(coef(lasso_dos), 4)
```

Gráfico de coeficientes Lasso

```{r}
lasso_coefs <- coef(lasso_dos) %>%
               as.matrix() %>%
               as_tibble(rownames = "predictor") %>%
               dplyr::rename(coeficiente = s0)

lasso_coefs %>%
  filter(predictor != "(Intercept)") %>%
  ggplot(aes(x = predictor, y = coeficiente)) +
  geom_col() +
  labs(title = "Coeficientes del modelo Ridge") +
  theme_bw() +
  theme(axis.text.x = element_text(size = 6, angle = 45)) +
  ylim(-0.6, 0.6)
```

Exportamos a jpg

```{r}
jpeg("coefs_lasso.jpeg", width = 12, height = 4, units = 'in', res = 600)
lasso_coefs <- coef(lasso_dos) %>%
               as.matrix() %>%
               as_tibble(rownames = "predictor") %>%
               dplyr::rename(coeficiente = s0)

lasso_coefs %>%
  filter(predictor != "(Intercept)") %>%
  ggplot(aes(x = predictor, y = coeficiente)) +
  geom_col() +
  labs(title = "Coeficientes del modelo Lasso") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(size = 6, angle = 45)) +
  ylim(-0.6, 0.6)
dev.off()
```


















Gráfico Trade-Off

```{r}
# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  RMSE = sqrt(SSE/nrow(df))
  bias = mean(predicted) - true
  var = RMSE - bias
}
}
```


```{r}
# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  RMSE = sqrt(SSE/nrow(df))
  bias = mean(predicted) - true
  var = RMSE - bias
}

nlam <- 401L
lam_seq <- seq(0, 100, leng = nlam)

for (i in 1:lam){
  mse[i] <- RMSE(1 - lam_seq[i])
  bias[i] <- 
}
#################################

nlam <- 401L
lambda_seq <- seq(0, 100, length = nlam)
mse <- matrix(0, nrow = nlam, ncol = 3)
gammahat <- matrix(0, nrow = nlam, ncol = ncol(Z))
for(i in 1:nlam){

#################################
n <- 50
shrink <- seq(0,0.5, length=n)
mse <- numeric(n)
bias <- numeric(n)
variance <- numeric(n)

for (i in 1:n) {
mse[i] <- MSE((1 - shrink[i]) * Z, mu)
bias[i] <- mu * shrink[i]
variance[i] <- (1 - shrink[i])^2
}
```

```{r}
MSE <- function(predicted, true, df) {
  return(sum(predicted - true)^2) / length(predicted)
}

for (i in 1:n) {
  mse[i] <- 
}

```

```{r}
#invisible(options(echo = TRUE))
mu <- 2

Z <- rnorm(20000, mu)

 MSE <- function(estimate, mu) {
 return(sum((estimate - mu)^2) / length(estimate))
 }

n <- 50
shrink <- seq(0,0.5, length=n)
mse <- numeric(n)
bias <- numeric(n)
variance <- numeric(n)

for (i in 1:n) {
mse[i] <- MSE((1 - shrink[i]) * Z, mu)
bias[i] <- mu * shrink[i]
variance[i] <- (1 - shrink[i])^2
}
 
 ###################################################
 
 #eval_results <- function(true, predicted, df) {
 # SSE <- sum((predicted - true)^2)
 # SST <- sum((true - mean(true))^2)
 # R_square <- 1 - SSE / SST
 # RMSE = sqrt(SSE/nrow(df))
```


```{r}
plot(shrink, mse, xlab='Shrinkage', ylab='MSE', type='l', col='green', lwd=3, lty=1, ylim=c(0,1.2))
lines(shrink, bias^2, col='red', lwd=3, lty=2)
lines(shrink, variance, col='blue', lwd=3, lty=2)
legend(0.02,0.6, c('Bias^2', 'Variance', 'MSE'), col=c('red', 'blue', 'green'), lwd=rep(3,3), lty=c(2,2,1))
#dev.off()
```

```{r}
# Function to compute MSE of ridge estimator
mse_ridge <- function(gamma, lambda, Z, sigmasq = 1){
  ZtZ <- crossprod(Z)
  p <- ncol(Z)
  W <- solve(ZtZ + lambda*diag(p))
  bias <- c((W %*% ZtZ - diag(p)) %*% gamma)
  varia <- sigmasq * diag( crossprod(Z %*% W))
  list(bias = bias, variance = varia, mse = sum(bias^2 + varia))
}
set.seed(9876)
# Create fake data
Z <- matrix(rnorm(n = 20*50, mean = 0, sd = 1), ncol = 20L)
# Center and renormalize Z
Z <- apply(Z, 2, scale)
# Create coefficient vector
gamma <- c(rep(0, 10), runif(10))

# Create sequence of lambda and matrix to store results
nlam <- 401
lambda_seq <- seq(0, 100, length = nlam)
mse <- matrix(0, nrow = nlam, ncol = 3)
gammahat <- matrix(0, nrow = nlam, ncol = ncol(Z))
for(i in 1:nlam){
  # evaluate bias + variance for each lambda
  mse_i <- mse_ridge(gamma = gamma, lambda = lambda_seq[i], Z = Z)
  gammahat[i,] <- gamma + mse_i$bias
  mse[i,1] <-  sum(mse_i$bias^2)
  mse[i,2] <-  sum(mse_i$variance)
  mse[i,3] <- mse_i$mse
}
# Plot the results as a function of lambda
matplot(lambda_seq, mse, type = "l", lty = 1, 
        bty = "l", xlab = expression(lambda), col = 3:1, 
        ylab = "Mean squared error decomposition")
abline(h = mse[1,3], lty = 2)
#abline(v = mse[1,3])
legend(x = "topleft", legend = c("sq. bias", "variance", "mse"), 
       col = 3:1, lty = 1, bty = "n")
```


```{r}
?abline
```



```{r}
# Function to compute MSE of ridge estimator
mse_ridge <- function(gamma, lambda, Z, sigmasq = 1){
  ZtZ <- crossprod(Z)
  p <- ncol(Z)
  W <- solve(ZtZ + lambda*diag(p))
  bias <- c((W %*% ZtZ - diag(p)) %*% gamma)
  varia <- sigmasq * diag( crossprod(Z %*% W))
  list(bias = bias, variance = varia, mse = sum(bias^2 + varia))
}
set.seed(9876)
# Create fake data
Z <- matrix(rnorm(n = 20*50, mean = 0, sd = 1), ncol = 20L)
# Center and renormalize Z
Z <- apply(Z, 2, scale)
# Create coefficient vector
gamma <- c(rep(0, 10), runif(10))

# Create sequence of lambda and matrix to store results
nlam <- 401
lambda_seq <- seq(0, 40, length = nlam)
mse <- matrix(0, nrow = nlam, ncol = 3)
gammahat <- matrix(0, nrow = nlam, ncol = ncol(Z))
for(i in 1:nlam){
  # evaluate bias + variance for each lambda
  mse_i <- mse_ridge(gamma = gamma, lambda = lambda_seq[i], Z = Z)
  gammahat[i,] <- gamma + mse_i$bias
  mse[i,1] <-  sum(mse_i$bias^2)
  mse[i,2] <-  sum(mse_i$variance)
  mse[i,3] <- mse_i$mse
}
# Plot the results as a function of lambda
matplot(lambda_seq, mse, type = "l", lty = 1, 
        bty = "l", xlab = expression(lambda), col = 3:1, 
        ylab = "Mean squared error decomposition")
abline(h = mse[1,3], lty = 2)
legend(x = "topleft", legend = c("sq. bias", "variance", "mse"), 
       col = 3:1, lty = 1, bty = "n")
```

VER: https://rpubs.com/Joaquin_AR/242707

```{r}

```



